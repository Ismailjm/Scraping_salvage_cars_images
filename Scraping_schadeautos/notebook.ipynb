{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "from selenium import webdriver # type: ignore\n",
    "from selenium.webdriver.chrome.service import Service # type: ignore\n",
    "from selenium.webdriver.common.by import By # type: ignore\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = 'https://www.schadeautos.nl/en/search/damaged/passenger-cars/1/1/0/0/0/0/1/0'\n",
    "driver.get(url)\n",
    "# Get the maximum number of pages\n",
    "\n",
    "max_page = driver.find_element(By.XPATH, '/html/body/section[2]/div/div/div[2]/div/div[1]/ul/li[13]/a').get_attribute('href')\n",
    "# max_page = max_page.text\n",
    "max_page = int(max_page.split(\"/\")[-1])\n",
    "car_posts = []\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(f\"Total pages: {max_page}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape all pages from the links of the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "car_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "# Create a lock\n",
    "lock = threading.Lock()\n",
    "\n",
    "def scrape_cars_posts(page):\n",
    "    url = f'https://www.schadeautos.nl/en/search/damaged/passenger-cars/1/1/0/0/0/0/1/{page}'\n",
    "    driver.get(url)\n",
    "    car_items = driver.find_elements(By.CSS_SELECTOR, '.car-inner.flexinner')\n",
    "\n",
    "    # Local list to store car data for this page\n",
    "    car_data_batch = []\n",
    "\n",
    "    # Collect links of the cars\n",
    "    for item in car_items:\n",
    "        title = item.find_element(By.TAG_NAME, 'h2')\n",
    "        car_link = title.find_element(By.TAG_NAME, 'a')\n",
    "        car_title = car_link.text\n",
    "        car_href = car_link.get_attribute(\"href\")\n",
    "        car_data_batch.append({'title': car_title, 'link': car_href})\n",
    "\n",
    "    # Acquire the lock before modifying the shared list\n",
    "    with lock:\n",
    "        car_data.extend(car_data_batch)\n",
    "\n",
    "    print(f\"Scraped {len(car_data_batch)} car posts from page {page}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Scraping car posts links for each page using multithreading\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(scrape_cars_posts, range(1, max_page + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data = pd.DataFrame(car_data)\n",
    "car_data.to_csv('./CSVs/car_data_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./CSVs/car_data_new.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(f\"Total cars: {len(df['link'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only the links that matches the makes in the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"./CSVs/car_data.csv\")\n",
    "\n",
    "# List of makes to filter (converted to lowercase)\n",
    "makes_to_keep = [\"dacia\", \"peugeot\", \"citroÃ«n\", \"renault\", \"ford\", \"toyota\", \n",
    "                 \"honda\", \"hyundai\", \"audi\", \"bmw\", \"volkswagen\", \"kia\", \n",
    "                 \"chevrolet\", \"mercedes\", \"nissan\", \"fiat\"]\n",
    "\n",
    "# Function to extract the make from the details\n",
    "def extract_make(details):\n",
    "    if pd.isna(details):\n",
    "        return None\n",
    "    words = details.split()\n",
    "    if len(words) > 1:\n",
    "        return words[0].lower()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to extract the make from each row and create a new column \"Make\"\n",
    "df[\"Make\"] = df[\"title\"].apply(extract_make)\n",
    "\n",
    "# Filter the DataFrame to include only the car posts with makes in the list (ignoring case)\n",
    "filtered_df = df[df['Make'].isin(makes_to_keep)]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv(\"./CSVs/filtered_car_posts.csv\", index=False)\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dropped_rows = len(df)-len(filtered_df)\n",
    "dropped_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "filter_dacia = filtered_df[filtered_df['Make'] == 'dacia']\n",
    "filter_dacia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "make_counts = filtered_df[\"Make\"].value_counts()\n",
    "sum_counts = make_counts.sum()\n",
    "sum_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break the dataset on small parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the filtered DataFrame\n",
    "filtered_df = pd.read_csv(\"./CSVs/filtered_car_posts.csv\")\n",
    "\n",
    "# Drop the first column\n",
    "filtered_df.drop(columns=filtered_df.columns[0], inplace=True)\n",
    "\n",
    "# Split the DataFrame into 10 parts\n",
    "filtered_df_parts = np.array_split(filtered_df, 5)\n",
    "\n",
    "# Iterate over each part and save it as a separate CSV file\n",
    "for i, part in enumerate(filtered_df_parts):\n",
    "    part.to_csv(f\"./CSVs/filtered_car_posts_{i + 1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape all the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create a new directory for each make if it doesn't exist\n",
    "if not os.path.exists(\"schadeautos\"):\n",
    "    os.makedirs(\"shcadeautos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./CSVs/car_data_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "img_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Create a lock\n",
    "lock = threading.Lock()\n",
    "\n",
    "def scrape_imgs_links(link):\n",
    "    # collecting images of each car\n",
    "    driver.get(link)\n",
    "    time.sleep(random.uniform(1, 5))\n",
    "\n",
    "    # select all the images\n",
    "    imgs = driver.find_elements(By.CSS_SELECTOR, '.thumbs img')\n",
    "    \n",
    "    # get all the links of the imgs\n",
    "    img_links_batch = []  \n",
    "    for img in imgs:\n",
    "        img_src = img.get_attribute('src')\n",
    "        img_links_batch.append(img_src)\n",
    "\n",
    "    # Acquire the lock before modifying the shared list\n",
    "    with lock:\n",
    "        img_links.extend(img_links_batch)\n",
    "    \n",
    "    print(f\"Downloaded {len(img_links_batch)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def scrape_images_threaded(links):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        executor.map(scrape_imgs_links, links)\n",
    "\n",
    "# Scrape the images for the first 10 links\n",
    "for link in df[\"link\"][:10]:  \n",
    "    scrape_images_threaded([link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_links_lock = pd.DataFrame(img_links, columns=['img_links'])\n",
    "img_links_lock.to_csv('./CSVs/img_links_lock.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_links_lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_links_lock = img_links_lock.drop_duplicates()\n",
    "img_links_lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_links_df = pd.DataFrame(img_links, columns=['img_links'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_links_df.to_csv(\"./CSVs/img_links.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(scrape_imgs_links, df['link'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
